---
title: "Homework 2"
subtitle: "Research Methods, Spring 2025"
author: "Ryan Scholte"
format:
  pdf:
    output-file: "Scholte-i-hw2-2"
    output-ext: "pdf"
    header-includes:
      - \usepackage{float}
      - \floatplacement{table}{H}
jupyter: python3
---
You can access the [Repository](https://github.com/rscholt/HW2/tree/40f3df4312fe9d68b7554fc1aaa383e4ac79a6ec/submission1)

```{python}
#| echo: false  # Hides code but keeps output
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#read in data
hcris_data=pd.read_csv('/Users/ryanscholte/Desktop/GitHub/HW2/data/output/HCRIS_Datac.csv')


# Convert 'fy_start' to datetime to extract the year
hcris_data['fy_start'] = pd.to_datetime(hcris_data['fy_start'], errors='coerce')
hcris_data['year'] = hcris_data['fy_start'].dt.year

# Count hospitals that filed more than one report in the same year
hospital_counts = hcris_data.groupby(['provider_number', 'year']).size().reset_index(name='report_count')
multiple_reports = hospital_counts[hospital_counts['report_count'] > 1]

# Count number of hospitals per year
hospitals_over_time = multiple_reports.groupby('year')['provider_number'].nunique()

#print(hospital_counts)
# Plot the results
plt.figure()
plt.plot(hospitals_over_time.index, hospitals_over_time.values)
plt.xlabel('Year')
plt.xticks(hospitals_over_time.index, rotation=45) 
plt.ylabel('Number of Hospitals')
plt.title('Hospitals Filing More Than One Report Per Year Over Time')
plt.grid(True)
plt.show()
```


# 2

```{python}
#| echo: false  # Hides code but keeps output
# Remove duplicate reports by keeping only the first occurrence per hospital per year
unique_hospitals = hcris_data.drop_duplicates(subset=['provider_number', 'year'])

# Count the number of unique hospital IDs (Medicare provider numbers)
unique_hospital_count = unique_hospitals['provider_number'].nunique()

# Display the result
print("Number of unique hospital IDs:", unique_hospital_count)

```

# 3

```{python}
#| echo: false  # Hides code but keeps output
import seaborn as sns
#Question 3
hcris_data['tot_charges'] = pd.to_numeric(hcris_data['tot_charges'], errors='coerce')
# Convert tot_charges to numeric
hcris_data['tot_charges'] = pd.to_numeric(hcris_data['tot_charges'], errors='coerce')
#Remove rows with missing charges or years, negative values, and outliers
charges_by_year = hcris_data[['year','tot_charges']].dropna ()
charges_by_year = charges_by_year [charges_by_year['tot_charges'] >= 0]
# Display summary statistics to find cutoff values
summary_stats = charges_by_year ['tot_charges']. describe ()
#print (summary_stats)
#creating upper bound limit
upper_bound = summary_stats['75%'] if '75%' in summary_stats else summary_stats['max']
charges_by_year = charges_by_year [charges_by_year['tot_charges'] <= upper_bound]
#charges_by_year['log_tot_charges'] = np.log(charges_by_year['tot_charges'] + 1)  # Adding 1 to avoid log(0)

# Plot violin plot
plt.figure(figsize=(12, 6)) 
sns.violinplot (x='year', y='tot_charges', data=charges_by_year)
plt.title("Total Charges by Year")
plt.xlabel ("Year")
plt.ylabel("Total Charges")
plt.xticks(rotation=45)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.show( )


```


# 4

```{python}
#| echo: false  # Hides code but keeps output
#Question 4
#Converting columns to numeric
numeric_columns = [
    'tot_discounts', 'tot_charges',
    'ip_charges', 'icu_charges', 'ancillary_charges',
    'tot_mcare_payment', 'tot_discharges', 'mcare_discharges'
]
hcris_data[numeric_columns] = hcris_data[numeric_columns].apply(pd.to_numeric, errors='coerce')
# Remove missing values
hcris_clean = hcris_data[['year'] + numeric_columns].dropna( )
# Calculate estimated price based on the formula
discount_factor = 1 - hcris_clean['tot_discounts'] / hcris_clean ['tot_charges']
price_num = (hcris_clean['ip_charges'] + hcris_clean['icu_charges'] + hcris_clean['ancillary_charges']) * discount_factor - hcris_clean[ 'tot_mcare_payment']
price_denom = hcris_clean['tot_discharges'] - hcris_clean ['mcare_discharges']
hcris_clean['estimated_price'] = price_num/price_denom
#removing outliers and negatives
hcris_clean = hcris_clean[hcris_clean['estimated_price'] > 0]

summary_stats = hcris_clean['estimated_price'].describe()
#print (summary_stats)
upper_bound = summary_stats ['75%'] if '75%' in summary_stats else summary_stats ['max']
hcris_clean = hcris_clean[hcris_clean['estimated_price'] <= upper_bound]
#Plot violin plot
plt.figure(figsize=(12, 6))
sns.violinplot(x='year', y='estimated_price', data=hcris_clean)
plt.title("Estimated Prices by Year")
plt.xlabel ("Year")
plt.ylabel("Estimated Price")
plt.xticks(rotation=45)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt. show()

```


# 5

```{python}
#| echo: false  # Hides code but keeps output
#Q5.

# Filter data to only include year 2012
hcris_2012 = hcris_data[hcris_data['year'] == 2012].copy()

# Calculate estimated price using the given formula
hcris_2012['discount_factor'] = 1 - (hcris_2012['tot_discounts'] / hcris_2012['tot_charges'])
hcris_2012['price_num'] = (hcris_2012['ip_charges'] + hcris_2012['icu_charges'] + hcris_2012['ancillary_charges']) * hcris_2012['discount_factor'] - hcris_2012['tot_mcare_payment']
hcris_2012['price_denom'] = hcris_2012['tot_discharges'] - hcris_2012['mcare_discharges']
hcris_2012['price'] = hcris_2012['price_num'] / hcris_2012['price_denom']

# Define penalty
hcris_2012['penalty'] = (hcris_2012['hrrp_payment'] + hcris_2012['hvbp_payment']) < 0

# Clean data
hcris_2012 = hcris_2012[(hcris_2012['price_denom'] > 100) & (hcris_2012['price_num'] > 0) & (hcris_2012['price'] > 0)]
hcris_2012 = hcris_2012[hcris_2012['beds'] > 30]
hcris_2012 = hcris_2012[hcris_2012['price'] < 100000]  # As in the R code

# NA payments
hcris_2012['hvbp_payment'] = hcris_2012['hvbp_payment'].fillna(0)
hcris_2012['hrrp_payment'] = hcris_2012['hrrp_payment'].fillna(0).abs()


# Calculate average price for penalized vs non-penalized hospitals
mean_penalized = round(hcris_2012.loc[hcris_2012['penalty'] == 1, 'price'].mean(), 2)
mean_non_penalized = round(hcris_2012.loc[hcris_2012['penalty'] == 0, 'price'].mean(), 2)

print(f"Mean price for penalized hospitals: {mean_penalized}")
print(f"Mean price for non-penalized hospitals: {mean_non_penalized}")

```

# 6

```{python}
#| echo: false  # Hides code but keeps output
#Q6
hcris_2012['beds_quartile'] = pd.qcut(hcris_2012['beds'], 4, labels=[1, 2, 3, 4])

# Create indicator variables for each quartile
for i in range(1, 5):
    hcris_2012[f'quartile_{i}'] = (hcris_2012['beds_quartile'] == i).astype(int)


# Calculate average price for treated and control groups within each quartile
Avg_per_group = []
for i in range(1, 5):
    treated_mean = hcris_2012.loc[(hcris_2012[f'quartile_{i}'] == 1) & (hcris_2012['penalty'] == 1), 'price'].mean()
    control_mean = hcris_2012.loc[(hcris_2012[f'quartile_{i}'] == 1) & (hcris_2012['penalty'] == 0), 'price'].mean()
    Avg_per_group.append({'Quartile': i, 'Treated_Mean_Price': round(treated_mean, 2), 'Control_Mean_Price': round(control_mean, 2)})

results_df = pd.DataFrame(Avg_per_group)
print(results_df)

```

\newpage

# 7a

```{python}
#| echo: false  # Hides code but keeps output
from causalinference import CausalModel
# Step 1: Define treatment (penalized) and control groups
hcris_2012['treated'] = (hcris_2012['penalty'] > 0).astype(int)
treated_df = hcris_2012[hcris_2012['treated'] == 1]
control_df = hcris_2012[hcris_2012['treated'] == 0]

# Step 2: Select bed quartiles for matching
covariate = 'beds_quartile'

# Step 3: Calculate inverse variance weights
variance_by_quartile = control_df.groupby(covariate)['beds'].var().fillna(1)
inverse_variance_weights = 1 / variance_by_quartile

# Step 4: Perform nearest neighbor matching using inverse variance distance
matched_pairs = []
for _, treated_row in treated_df.iterrows():
    quartile = treated_row[covariate]
    control_candidates = control_df[control_df[covariate] == quartile]

    if not control_candidates.empty:
        # Compute distance: absolute difference in beds * inverse variance weight
        distances = np.abs(control_candidates['beds'] - treated_row['beds']) * inverse_variance_weights[quartile]

        # Get the best match (hospital with minimum distance)
        best_match_idx = distances.idxmin()
        best_match = control_candidates.loc[best_match_idx]

        # Store matched pair (treated price, control price)
        matched_pairs.append((treated_row['price'], best_match['price']))

# Step 5: Compute ATE (Average Treatment Effect)
treated_prices, control_prices = zip(*matched_pairs)
ate_nn_inverse_variance = np.mean(np.array(treated_prices) - np.array(control_prices))

# Step 6: Use CausalModel for ATE estimation (bias adjustment)
X = hcris_2012[[covariate]].values  # Use quartiles as covariates
y = hcris_2012['price'].values
treatment = hcris_2012['treated'].values

causal_model = CausalModel(Y=y, D=treatment, X=X)
causal_model.est_via_matching(matches=1, bias_adj=True)  # No weight argument needed!

# Step 7: Print the ATE estimates
print(f"ATE using Nearest Neighbor Matching (Inverse Variance Distance): {ate_nn_inverse_variance:.4f}")
print(causal_model.estimates)

```


# 7b

```{python}
#| echo: false  # Hides code but keeps output
# Create quartiles for bed size
hcris_2012['bed_quartile'] = pd.qcut(hcris_2012['beds'], 4, labels=False)

# Step 2: Select relevant variables
X = hcris_2012[['bed_quartile']].values  # Matching is based on quartiles
y = hcris_2012['price'].values  # Outcome variable (penalty amount)
treatment = hcris_2012['penalty'].values  # Treatment indicator (1 = Penalized, 0 = Not Penalized)

# Step 3: Create Causal Model
causal_model_mahal = CausalModel(Y=y, D=treatment, X=X)

# Step 4: Perform Nearest Neighbor Matching (1-to-1) with Inverse Variance Distance
causal_model_mahal.est_via_matching(matches=1, bias_adj=True)

# Step 4: Print ATE Results
print("ATE using Nearest Neighbor Matching (Mahalanobis Distance):")
print(causal_model_mahal.estimates)

```

\newpage

# 7c

```{python}
#| echo: false  # Hides code but keeps output

# Select relevant covariates for propensity score estimation
X = hcris_2012[['bed_quartile']].values
y = hcris_2012['price'].values  # Outcome variable
treatment = hcris_2012['penalty'].values  # Treatment indicator

# Create Causal Model
causal_model_ps = CausalModel(Y=y, D=treatment, X=X)

# Estimate Propensity Scores using the built-in method
causal_model_ps.est_propensity()

# Perform Nearest Neighbor Matching (1-to-1) based on Propensity Score
causal_model_ps.est_via_matching(matches=1, bias_adj=True)

# Print ATE Results
print("ATE using Propensity Score Matching:")
print(causal_model_ps.estimates)

```


# 7d

```{python}
#| echo: false  # Hides code but keeps output
# Define the treatment variable (1 = Penalized, 0 = Not Penalized)
hcris_2012['treated'] = (hcris_2012['penalty'] > 0).astype(int)

# Select covariates for propensity score estimation
X = hcris_2012[['beds', 'mcaid_discharges', 'ip_charges', 'mcare_discharges', 'tot_mcare_payment']].values
y = hcris_2012['price'].values
treatment = hcris_2012['treated'].values

# Create Causal Model
causal_model = CausalModel(Y=y, D=treatment, X=X)

# Step 1: Estimate Propensity Scores
causal_model.est_propensity()
ps_scores = causal_model.propensity['fitted']  # Extract estimated propensity scores

# Step 2: Compute Inverse Propensity Weights (IPW)
hcris_2012['ipw'] = np.where(
    hcris_2012['treated'] == 1, 1 / ps_scores, 1 / (1 - ps_scores)
)

# Step 3: Compute Weighted Means
mean_treated = np.average(hcris_2012.loc[hcris_2012['treated'] == 1, 'price'], weights=hcris_2012.loc[hcris_2012['treated'] == 1, 'ipw'])
mean_control = np.average(hcris_2012.loc[hcris_2012['treated'] == 0, 'price'], weights=hcris_2012.loc[hcris_2012['treated'] == 0, 'ipw'])

# Step 4: Compute ATE using IPW
ate_ipw = mean_treated - mean_control
print(f"ATE using IPW: {ate_ipw:.4f}")

# Create quartile-based dummy variables for 'beds'
hcris_2012['beds_quartile'] = pd.qcut(hcris_2012['beds'], 4, labels=False)

# Manually create dummy variables (One-Hot Encoding, excluding first quartile)
for i in range(1, 4):  # Exclude first quartile to avoid multicollinearity
    hcris_2012[f'beds_quartile_{i+1}'] = (hcris_2012['beds_quartile'] == i).astype(int)

# Create interaction terms between 'penalty' and key covariates
hcris_2012['beds_diff'] = hcris_2012['penalty'] * (hcris_2012['beds'] - hcris_2012['beds'].mean())
hcris_2012['mcaid_diff'] = hcris_2012['penalty'] * (hcris_2012['mcaid_discharges'] - hcris_2012['mcaid_discharges'].mean())
hcris_2012['ip_diff'] = hcris_2012['penalty'] * (hcris_2012['ip_charges'] - hcris_2012['ip_charges'].mean())
hcris_2012['mcare_diff'] = hcris_2012['penalty'] * (hcris_2012['mcare_discharges'] - hcris_2012['mcare_discharges'].mean())
hcris_2012['mpay_diff'] = hcris_2012['penalty'] * (hcris_2012['tot_mcare_payment'] - hcris_2012['tot_mcare_payment'].mean())



# Drop NaN rows from original dataset before extracting X_reg and y_reg
hcris_2012_filtered = hcris_2012.dropna(subset=['penalty', 'beds', 'mcaid_discharges', 'ip_charges', 
                                                 'mcare_discharges', 'tot_mcare_payment', 'price',
                                                 'beds_diff', 'mcaid_diff', 'ip_diff', 'mcare_diff', 'mpay_diff',
                                                 'beds_quartile_2', 'beds_quartile_3', 'beds_quartile_4'])

# Convert all columns to float
X_reg = hcris_2012_filtered[['penalty', 'beds', 'mcaid_discharges', 'ip_charges', 
                             'mcare_discharges', 'tot_mcare_payment', 
                             'beds_diff', 'mcaid_diff', 'ip_diff', 'mcare_diff', 'mpay_diff', 
                             'beds_quartile_2', 'beds_quartile_3', 'beds_quartile_4']].astype(float).values

y_reg = hcris_2012_filtered['price'].astype(float).values

# Solve for beta coefficients using OLS: Î² = (X'X)^(-1) X'y
XTX_inv = np.linalg.pinv(X_reg.T @ X_reg)  # Use pseudoinverse to handle singularity
XTy = X_reg.T @ y_reg
beta = XTX_inv @ XTy  # Compute beta coefficients

# Print Coefficients
print("Linear Regression Coefficients:")
print(dict(zip(['penalty', 'beds', 'mcaid_discharges', 'ip_charges', 'mcare_discharges', 'tot_mcare_payment',
                'beds_diff', 'mcaid_diff', 'ip_diff', 'mcare_diff', 'mpay_diff',
                'beds_quartile_2', 'beds_quartile_3', 'beds_quartile_4'], beta)))

# **Fix: Use the Same Filtered Data for Indexing**
pred_treated = X_reg[hcris_2012_filtered['penalty'] == 1] @ beta
pred_control = X_reg[hcris_2012_filtered['penalty'] == 0] @ beta

# Ensure same length for treated and control before computing ATE
n = min(len(pred_treated), len(pred_control))
ate_regression = np.mean(pred_treated[:n] - pred_control[:n])

# Print ATE Result
print(f"ATE using Linear Regression: {ate_regression:.4f}")

```



\newpage

# 8
all ATE are the same

# 9
no, I dont think my data and code is correct yet. But even with that I think there are many other issues we talked about that we cant assume to determine the causality. those can be unobserved factors like patient acuity that affect the chance of a hospital being penailized. This means penalized hospitals arent assigned randomly and have other factors that are not fully controlled for. 


# 10
It was very difficult especially if the cleaning is not done correctly. Also using a bunch new packages is tough. I learned that file paths are super frustrating and can change all the time but there a bunch of differeent solutions and I haven't found a consistent or one I have learned to use consistently yet. 